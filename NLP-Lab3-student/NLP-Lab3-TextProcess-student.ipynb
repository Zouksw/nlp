{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83b33b86",
   "metadata": {},
   "source": [
    "# 华东师范大学 计算机科学与技术学院 上机练习\n",
    "\n",
    "\n",
    "## 基本预处理过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab55c099",
   "metadata": {},
   "source": [
    "### 给定一个文本文件，统计单词和词频"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33126c1",
   "metadata": {},
   "source": [
    "* 导入所需要的库"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfb13b5",
   "metadata": {},
   "source": [
    "* 打开原始数据文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96c021c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10061]\n",
      "[nltk_data]     由于目标计算机积极拒绝，无法连接。>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk as tk\n",
    "tk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53247bea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n"
     ]
    }
   ],
   "source": [
    "with open (r'data\\0_9.txt','r') as file:\n",
    "    data=file.read()\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eec62c",
   "metadata": {},
   "source": [
    "* 将所有非数字字符的符号转化为空，大小写转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84083424",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell highs satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled  at  high a classic line inspector im here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isnt\n"
     ]
    }
   ],
   "source": [
    "# 更新你的data，把大写转为小写\n",
    "data = data.lower()\n",
    "# 将所有非数字字符的符号转化为空\n",
    "data = re.sub(r'[^a-z0-9\\s]', '', data)  \n",
    "print(data)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99282d9e",
   "metadata": {},
   "source": [
    "* 文本标记化/分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c686a59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\86187/nltk_data'\n    - 'f:\\\\anaconda\\\\envs\\\\skim\\\\nltk_data'\n    - 'f:\\\\anaconda\\\\envs\\\\skim\\\\share\\\\nltk_data'\n    - 'f:\\\\anaconda\\\\envs\\\\skim\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\86187\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'F:/anaconda/envs/skim/lib/nltk_data'\n    - 'F:/anaconda/envs/skim/lib/nltk_data'\n    - 'F:/anaconda/envs/skim/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m tk.data.path.append(\u001b[33m'\u001b[39m\u001b[33mF:/anaconda/envs/skim/lib/nltk_data\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m tokens = tk.wordpunct_tokenize(data)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m tokens2 = \u001b[43mtk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(tokens2)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(tokens)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\anaconda\\envs\\skim\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\anaconda\\envs\\skim\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\anaconda\\envs\\skim\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\anaconda\\envs\\skim\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\anaconda\\envs\\skim\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\anaconda\\envs\\skim\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\86187/nltk_data'\n    - 'f:\\\\anaconda\\\\envs\\\\skim\\\\nltk_data'\n    - 'f:\\\\anaconda\\\\envs\\\\skim\\\\share\\\\nltk_data'\n    - 'f:\\\\anaconda\\\\envs\\\\skim\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\86187\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'F:/anaconda/envs/skim/lib/nltk_data'\n    - 'F:/anaconda/envs/skim/lib/nltk_data'\n    - 'F:/anaconda/envs/skim/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# 获得你的tokens #\n",
    "tk.data.path.append('F:/anaconda/envs/skim/lib/nltk_data')\n",
    "tokens = tk.wordpunct_tokenize(data)\n",
    "tokens2 = tk.word_tokenize(data)\n",
    "print(tokens2)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9d22e0",
   "metadata": {},
   "source": [
    "* 去停用词\n",
    "\n",
    "* 运行代码之前确保已经下载了\"stopwords\"资源\n",
    "run  nltk.download('stopwords') \n",
    "\n",
    "当进行自然语言处理（NLP）任务时，常常需要处理停用词（stopwords）。停用词是指在文本中频繁出现但通常没有实际含义或对任务没有贡献的常见词语。例如，在英语中，“the”、“is”、“and”等单词经常出现，但它们通常不携带重要的语义信息。\n",
    "\n",
    "NLTK库中的stopwords模块提供了一个预定义的停用词列表，可以用于从文本中去除这些常见词语。这个列表包含了多种语言的停用词，包括英语、法语、西班牙语等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed93e497",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\86187\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "tk.download('stopwords')\n",
    "# 对你的tokens去停用词 #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c98acc7",
   "metadata": {},
   "source": [
    "* 词性标注\n",
    "\n",
    "`tokens_pos` 是通过 NLTK 库中的 `pos_tag` 函数对 `tokens` 进行词性标注后得到的结果。词性标注将每个单词标注为其在句子中的词性。\n",
    "\n",
    "在英语中，常见的词性标记如下：\n",
    "\n",
    "- CC: Coordinating conjunction（并列连词）\n",
    "- CD: Cardinal number（基数词）\n",
    "- DT: Determiner（限定词）\n",
    "- EX: Existential there（存在句）\n",
    "- FW: Foreign word（外来词）\n",
    "- IN: Preposition or subordinating conjunction（介词或从属连词）\n",
    "- JJ: Adjective（形容词）\n",
    "- JJR: Adjective, comparative（形容词，比较级）\n",
    "- JJS: Adjective, superlative（形容词，最高级）\n",
    "- LS: List item marker（列表项标记）\n",
    "- MD: Modal（情态动词）\n",
    "- NN: Noun, singular or mass（名词，单数或不可数名词）\n",
    "- NNS: Noun, plural（名词，复数）\n",
    "- NNP: Proper noun, singular（专有名词，单数）\n",
    "- NNPS: Proper noun, plural（专有名词，复数）\n",
    "- PDT: Predeterminer（前位限定词）\n",
    "- POS: Possessive ending（所有格结束词）\n",
    "- PRP: Personal pronoun（人称代词）\n",
    "- RB: Adverb（副词）\n",
    "- RBR: Adverb, comparative（副词，比较级）\n",
    "- RBS: Adverb, superlative（副词，最高级）\n",
    "- RP: Particle（小品词）\n",
    "- SYM: Symbol（符号）\n",
    "- TO: to（不定式标记）\n",
    "- UH: Interjection（感叹词）\n",
    "- VB: Verb, base form（动词，基本形式）\n",
    "- VBD: Verb, past tense（动词，过去式）\n",
    "- VBG: Verb, gerund or present participle（动词，动名词或现在分词）\n",
    "- VBN: Verb, past participle（动词，过去分词）\n",
    "- VBP: Verb, non-3rd person singular present（动词，非第三人称单数现在时）\n",
    "- VBZ: Verb, 3rd person singular present（动词，第三人称单数现在时）\n",
    "- WDT: Wh-determiner（Wh 限定词）\n",
    "- WP: Wh-pronoun（Wh 代词）\n",
    "- WRB: Wh-adverb（Wh 副词）\n",
    "\n",
    "以上是一些常见的英语词性标记。根据输入的 `tokens` 内容，`tokens_pos` 的输出将显示每个单词及其对应的词性标记。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5045728",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\86187\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.download('averaged_perceptron_tagger')\n",
    "# 获得tokens_pos并打印 #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e90d39",
   "metadata": {},
   "source": [
    "* 词干提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21015fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "result=[]\n",
    "# 把tokens中的每一个token加入result #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab14eabe",
   "metadata": {},
   "source": [
    "* 词形还原"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5535632f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\86187\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk.stem as ns\n",
    "tk.download('wordnet')\n",
    "lemmatizer = ns.WordNetLemmatizer()\n",
    "result = []\n",
    "# result.append(lemmatizer.lemmatize(token, pos=postag))  token是具体的单词，如果posttag不改则可能输出多种词性\n",
    "# 常见的postag有 n 名词, v 动词 , a 形容词, r 副词\n",
    " # 将 \"postag\" 替换为具体的词性标记符号\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41ec58f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBG\n",
      "v\n"
     ]
    }
   ],
   "source": [
    "import nltk.stem as ns\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "\n",
    "# 定义要进行词形还原的单词\n",
    "token = \"running\"\n",
    "\n",
    "result = []\n",
    "lemmatizer = ns.WordNetLemmatizer()\n",
    "\n",
    "# 获取单词的词性标记，使用pos_tag函数\n",
    "pos = pos_tag([token])[0][1]\n",
    "# pos = pos_tag([token]) 会打印出什么？尝试一下\n",
    "print(pos)\n",
    "\n",
    "# 将词性标记转换为WordNet词性标记\n",
    "wn_pos = wordnet.VERB if pos.startswith('V') else wordnet.NOUN\n",
    "\n",
    "print(wn_pos)\n",
    "# 现在可以使用lemmatizer.lemmatize()还原词性并且打印结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b1535d",
   "metadata": {},
   "source": [
    "* 统计排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd7418f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('run', 1)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回出现频率最高的100个字符，使用刚才获得的result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c721a3b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 基于词典的分词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df7c7d6",
   "metadata": {},
   "source": [
    "编程实现正向和逆向最大匹配分词算法。 \n",
    "\n",
    "已知：词典={“我们”, “在”, “玩”, “在野”，“野生”“生动”“动物”, “动\n",
    "物园”，”野生动物园”} \n",
    "\n",
    "输入：“我们在野生动物园玩” \n",
    "输出：分词结果的列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7295eec2-f8a8-48de-98e7-2a332f6797ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

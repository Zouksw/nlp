{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0cca2cb",
   "metadata": {},
   "source": [
    "# 华东师范大学 计算机科学与技术学院 上机练习\n",
    "\n",
    "\n",
    "## 基本预处理过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577d4f46",
   "metadata": {},
   "source": [
    "### 给定一个文本文件，统计单词和词频"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e303332a",
   "metadata": {},
   "source": [
    "* 导入所需要的库"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909b1748",
   "metadata": {},
   "source": [
    "* 打开原始数据文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2af128ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\86187\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\86187\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk as tk\n",
    "tk.download('punkt')\n",
    "tk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4bb27ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n"
     ]
    }
   ],
   "source": [
    "with open (r'data\\0_9.txt','r') as file:\n",
    "    data=file.read()\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5f508d",
   "metadata": {},
   "source": [
    "* 将所有非数字字符的符号转化为空，大小写转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0428b601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell highs satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled  at  high a classic line inspector im here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isnt\n"
     ]
    }
   ],
   "source": [
    "# 更新你的data，把大写转为小写\n",
    "data = data.lower()\n",
    "# 将所有非数字字符的符号转化为空\n",
    "data = re.sub(r'[^a-z0-9\\s]', '', data)  \n",
    "print(data)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1cdf09",
   "metadata": {},
   "source": [
    "* 文本标记化/分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93dfa839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', 'such', 'as', 'teachers', 'my', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'bromwell', 'highs', 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', 'teachers', 'the', 'scramble', 'to', 'survive', 'financially', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', 'pomp', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', 'i', 'immediately', 'recalled', 'at', 'high', 'a', 'classic', 'line', 'inspector', 'im', 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', 'student', 'welcome', 'to', 'bromwell', 'high', 'i', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'bromwell', 'high', 'is', 'far', 'fetched', 'what', 'a', 'pity', 'that', 'it', 'isnt']\n",
      "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', 'such', 'as', 'teachers', 'my', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'bromwell', 'highs', 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', 'teachers', 'the', 'scramble', 'to', 'survive', 'financially', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', 'pomp', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', 'i', 'immediately', 'recalled', 'at', 'high', 'a', 'classic', 'line', 'inspector', 'im', 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', 'student', 'welcome', 'to', 'bromwell', 'high', 'i', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'bromwell', 'high', 'is', 'far', 'fetched', 'what', 'a', 'pity', 'that', 'it', 'isnt']\n"
     ]
    }
   ],
   "source": [
    "# 获得你的tokens #\n",
    "tokens = tk.wordpunct_tokenize(data)\n",
    "tokens2 = tk.word_tokenize(data)\n",
    "print(tokens2)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6b88a8",
   "metadata": {},
   "source": [
    "* 去停用词\n",
    "\n",
    "* 运行代码之前确保已经下载了\"stopwords\"资源\n",
    "run  nltk.download('stopwords') \n",
    "\n",
    "当进行自然语言处理（NLP）任务时，常常需要处理停用词（stopwords）。停用词是指在文本中频繁出现但通常没有实际含义或对任务没有贡献的常见词语。例如，在英语中，“the”、“is”、“and”等单词经常出现，但它们通常不携带重要的语义信息。\n",
    "\n",
    "NLTK库中的stopwords模块提供了一个预定义的停用词列表，可以用于从文本中去除这些常见词语。这个列表包含了多种语言的停用词，包括英语、法语、西班牙语等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "319ca211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\86187\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "tk.download('stopwords')\n",
    "# 对你的tokens去停用词 #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8374dc84",
   "metadata": {},
   "source": [
    "* 词性标注\n",
    "\n",
    "`tokens_pos` 是通过 NLTK 库中的 `pos_tag` 函数对 `tokens` 进行词性标注后得到的结果。词性标注将每个单词标注为其在句子中的词性。\n",
    "\n",
    "在英语中，常见的词性标记如下：\n",
    "\n",
    "- CC: Coordinating conjunction（并列连词）\n",
    "- CD: Cardinal number（基数词）\n",
    "- DT: Determiner（限定词）\n",
    "- EX: Existential there（存在句）\n",
    "- FW: Foreign word（外来词）\n",
    "- IN: Preposition or subordinating conjunction（介词或从属连词）\n",
    "- JJ: Adjective（形容词）\n",
    "- JJR: Adjective, comparative（形容词，比较级）\n",
    "- JJS: Adjective, superlative（形容词，最高级）\n",
    "- LS: List item marker（列表项标记）\n",
    "- MD: Modal（情态动词）\n",
    "- NN: Noun, singular or mass（名词，单数或不可数名词）\n",
    "- NNS: Noun, plural（名词，复数）\n",
    "- NNP: Proper noun, singular（专有名词，单数）\n",
    "- NNPS: Proper noun, plural（专有名词，复数）\n",
    "- PDT: Predeterminer（前位限定词）\n",
    "- POS: Possessive ending（所有格结束词）\n",
    "- PRP: Personal pronoun（人称代词）\n",
    "- RB: Adverb（副词）\n",
    "- RBR: Adverb, comparative（副词，比较级）\n",
    "- RBS: Adverb, superlative（副词，最高级）\n",
    "- RP: Particle（小品词）\n",
    "- SYM: Symbol（符号）\n",
    "- TO: to（不定式标记）\n",
    "- UH: Interjection（感叹词）\n",
    "- VB: Verb, base form（动词，基本形式）\n",
    "- VBD: Verb, past tense（动词，过去式）\n",
    "- VBG: Verb, gerund or present participle（动词，动名词或现在分词）\n",
    "- VBN: Verb, past participle（动词，过去分词）\n",
    "- VBP: Verb, non-3rd person singular present（动词，非第三人称单数现在时）\n",
    "- VBZ: Verb, 3rd person singular present（动词，第三人称单数现在时）\n",
    "- WDT: Wh-determiner（Wh 限定词）\n",
    "- WP: Wh-pronoun（Wh 代词）\n",
    "- WRB: Wh-adverb（Wh 副词）\n",
    "\n",
    "以上是一些常见的英语词性标记。根据输入的 `tokens` 内容，`tokens_pos` 的输出将显示每个单词及其对应的词性标记。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "940c46d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\86187\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\86187\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.download('averaged_perceptron_tagger')\n",
    "tk.download('averaged_perceptron_tagger_eng')\n",
    "# 获得tokens_pos并打印 #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d110e285",
   "metadata": {},
   "source": [
    "* 词干提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5adf5539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "result=[]\n",
    "# 把tokens中的每一个token加入result #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c2602a",
   "metadata": {},
   "source": [
    "* 词形还原"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00362515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\86187\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk.stem as ns\n",
    "tk.download('wordnet')\n",
    "lemmatizer = ns.WordNetLemmatizer()\n",
    "result = []\n",
    "# result.append(lemmatizer.lemmatize(token, pos=postag))  token是具体的单词，如果posttag不改则可能输出多种词性\n",
    "# 常见的postag有 n 名词, v 动词 , a 形容词, r 副词\n",
    " # 将 \"postag\" 替换为具体的词性标记符号\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4766ef23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBG\n",
      "v\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "import nltk.stem as ns\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "\n",
    "# 定义要进行词形还原的单词\n",
    "token = \"running\"\n",
    "\n",
    "result = []\n",
    "lemmatizer = ns.WordNetLemmatizer()\n",
    "\n",
    "# 获取单词的词性标记，使用pos_tag函数\n",
    "pos = pos_tag([token])[0][1]\n",
    "# pos = pos_tag([token]) 会打印出什么？尝试一下\n",
    "print(pos)\n",
    "\n",
    "# 将词性标记转换为WordNet词性标记\n",
    "wn_pos = wordnet.VERB if pos.startswith('V') else wordnet.NOUN\n",
    "\n",
    "print(wn_pos)\n",
    "# 现在可以使用lemmatizer.lemmatize()还原词性并且打印结果\n",
    "result = lemmatizer.lemmatize(token, pos=wn_pos)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f37107",
   "metadata": {},
   "source": [
    "* 统计排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94ea6439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'r': 1\n",
      "'u': 1\n",
      "'n': 1\n"
     ]
    }
   ],
   "source": [
    "# 返回出现频率最高的100个字符，使用刚才获得的result\n",
    "from collections import Counter\n",
    "\n",
    "char_counts = Counter(result)\n",
    "\n",
    "top_100_chars = char_counts.most_common(100)\n",
    "\n",
    "for char, count in top_100_chars:\n",
    "    print(f\"'{char}': {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e4fb2a",
   "metadata": {},
   "source": [
    "### 基于词典的分词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05687c72",
   "metadata": {},
   "source": [
    "编程实现正向和逆向最大匹配分词算法。 \n",
    "\n",
    "已知：词典={“我们”, “在”, “玩”, “在野”，“野生”“生动”“动物”, “动\n",
    "物园”，”野生动物园”} \n",
    "\n",
    "输入：“我们在野生动物园玩” \n",
    "输出：分词结果的列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dffa7a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正向最大匹配: ['我们', '在野', '生动', '物', '园', '玩']\n",
      "逆向最大匹配: ['我们', '在', '野生动物园', '玩']\n"
     ]
    }
   ],
   "source": [
    "def fmm(text, vocab, max_len):\n",
    "    \"\"\"正向最大匹配\"\"\"\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        matched = False\n",
    "        for l in range(min(max_len, len(text) - i), 0, -1):\n",
    "            word = text[i:i + l]\n",
    "            if word in vocab:\n",
    "                result.append(word)\n",
    "                i += l\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            result.append(text[i])\n",
    "            i += 1\n",
    "    return result\n",
    "\n",
    "def bmm(text, vocab, max_len):\n",
    "    \"\"\"逆向最大匹配\"\"\"\n",
    "    result = []\n",
    "    i = len(text)\n",
    "    while i > 0:\n",
    "        matched = False\n",
    "        for l in range(min(max_len, i), 0, -1):\n",
    "            word = text[i - l:i]\n",
    "            if word in vocab:\n",
    "                result.append(word)\n",
    "                i -= l\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            result.append(text[i - 1])\n",
    "            i -= 1\n",
    "    return result[::-1] \n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "    vocab = {\"我们\", \"在\", \"玩\", \"在野\", \"野生\", \"生动\", \"动物\", \"动物园\", \"野生动物园\"}\n",
    "    text = \"我们在野生动物园玩\"\n",
    "    max_len = max(len(w) for w in vocab)\n",
    "\n",
    "    fmm_result = fmm(text, vocab, max_len)\n",
    "    bmm_result = bmm(text, vocab, max_len)\n",
    "\n",
    "    print(\"正向最大匹配:\", fmm_result)\n",
    "    print(\"逆向最大匹配:\", bmm_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11364cd9",
   "metadata": {},
   "source": [
    "### 给定一个文本文件，统计单词和词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cc7abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words (with NLTK):\n",
      "the: 23\n",
      "and: 20\n",
      "to: 16\n",
      "for: 14\n",
      "memory: 14\n",
      "of: 13\n",
      "infllm: 11\n",
      "sequences: 11\n",
      "llms: 9\n",
      "with: 8\n",
      "long: 7\n",
      "units: 7\n",
      "use: 7\n",
      "as: 6\n",
      "a: 6\n",
      "in: 6\n",
      "can: 6\n",
      "attention: 6\n",
      "gpu: 5\n",
      "usage: 5\n",
      "topk: 5\n",
      "tokens: 5\n",
      "we: 5\n",
      "you: 5\n",
      "intrinsic: 4\n",
      "extremely: 4\n",
      "code: 4\n",
      "paper: 4\n",
      "overview: 4\n",
      "length: 4\n",
      "is: 4\n",
      "configuration: 4\n",
      "model: 4\n",
      "type: 4\n",
      "false: 4\n",
      "number: 4\n",
      "unveiling: 3\n",
      "capacity: 3\n",
      "understanding: 3\n",
      "https: 3\n",
      "faiss: 3\n",
      "requirements: 3\n",
      "citation: 3\n",
      "on: 3\n",
      "not: 3\n",
      "these: 3\n",
      "dependencies: 3\n",
      "files: 3\n",
      "by: 3\n",
      "block: 3\n",
      "lru: 3\n",
      "be: 3\n",
      "will: 3\n",
      "only: 3\n",
      "input: 3\n",
      "truncation: 3\n",
      "following: 3\n",
      "our: 2\n",
      "march: 2\n",
      "see: 2\n",
      "inference: 2\n",
      "april: 2\n",
      "supports: 2\n",
      "retrieval: 2\n",
      "streaming: 2\n",
      "existing: 2\n",
      "maximum: 2\n",
      "issues: 2\n",
      "sliding: 2\n",
      "distant: 2\n",
      "achieve: 2\n",
      "capture: 2\n",
      "ability: 2\n",
      "process: 2\n",
      "an: 2\n",
      "computation: 2\n",
      "training: 2\n",
      "datasets: 2\n",
      "yaml: 2\n",
      "or: 2\n",
      "path: 2\n",
      "base: 2\n",
      "settings: 2\n",
      "local: 2\n",
      "time: 2\n",
      "execution: 2\n",
      "used: 2\n",
      "first: 2\n",
      "accelerate: 2\n",
      "it: 2\n",
      "perhead: 2\n",
      "if: 2\n",
      "suffix: 2\n",
      "evaluation: 2\n",
      "infinitebench: 2\n",
      "longbench: 2\n",
      "running: 2\n",
      "command: 2\n",
      "bash: 2\n",
      "evaluate: 2\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "def count_words_nltk(file_path, top_n=100, remove_stopwords=False, lang='english'):\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # 1. 分词（保留标点，后续过滤）\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # 2. 只保留字母组成的单词（过滤标点、数字等）\n",
    "    words = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "    # 3. （可选）移除停用词\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words(lang))\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # 4. 统计词频\n",
    "    word_counts = Counter(words)\n",
    "    return word_counts.most_common(top_n)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"./README.md\"\n",
    "\n",
    "    top_words = count_words_nltk(file_path, top_n=100)\n",
    "\n",
    "    print(\"Top words:\")\n",
    "    for word, freq in top_words:\n",
    "        print(f\"{word}: {freq}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
